import numpy as np
import pandas as pd
from sklearn.linear_model import LinearRegression
import os
import shutil
from src.evaluate_model import error_analysis, load_model_from_mlflow, load_data_from_mlflow

def create_test_data():
    """CrÃ©e des donnÃ©es synthÃ©tiques pour les tests"""
    np.random.seed(42)

    # CrÃ©ation des features
    X_train = pd.DataFrame({
        'feature1': np.random.normal(0, 1, 100),
        'feature2': np.random.normal(0, 1, 100)
    })
    X_test = pd.DataFrame({
        'feature1': np.random.normal(0, 1, 30),
        'feature2': np.random.normal(0, 1, 30)
    })

    # CrÃ©ation des variables cibles avec un peu de bruit
    y_train = pd.DataFrame(X_train['feature1'] * 2 + X_train['feature2'] + np.random.normal(0, 0.1, 100))
    y_test = pd.DataFrame(X_test['feature1'] * 2 + X_test['feature2'] + np.random.normal(0, 0.1, 30))

    # CrÃ©ation et entraÃ®nement du modÃ¨le
    model = LinearRegression()
    model.fit(X_train, y_train)

    return X_train, X_test, y_train, y_test, model

def test_error_analysis_output_structure():
    """Teste la structure de sortie de l'analyse d'erreurs"""
    print("\nğŸ§ª Test de la structure des mÃ©triques...")

    # PrÃ©paration
    X_train, X_test, y_train, y_test, model = create_test_data()
    output_dir = "test_artifacts"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # ExÃ©cution
        metrics = error_analysis(model, X_train, y_train, X_test, y_test, output_dir)

        # VÃ©rification
        expected_metrics = ['Train RMSE', 'Train MAE', 'Train R2',
                          'Test RMSE', 'Test MAE', 'Test R2']

        all_metrics_present = all(metric in metrics for metric in expected_metrics)
        all_metrics_float = all(isinstance(metrics[metric], float) for metric in expected_metrics)

        assert all_metrics_present, "âŒ Certaines mÃ©triques sont manquantes"
        assert all_metrics_float, "âŒ Certaines mÃ©triques ne sont pas des nombres"

        print("âœ… Test de structure rÃ©ussi")
    finally:
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)

def test_file_creation():
    """Teste la crÃ©ation des fichiers de visualisation"""
    print("\nğŸ§ª Test de la crÃ©ation des fichiers...")

    # PrÃ©paration
    X_train, X_test, y_train, y_test, model = create_test_data()
    output_dir = "test_artifacts"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # ExÃ©cution
        error_analysis(model, X_train, y_train, X_test, y_test, output_dir)

        # VÃ©rification
        expected_files = [
            'train_predictions.png',
            'test_predictions.png',
            'train_error_dist.png',
            'test_error_dist.png'
        ]

        for file in expected_files:
            assert os.path.exists(os.path.join(output_dir, file)), f"âŒ Le fichier {file} n'existe pas"

        print("âœ… Test de crÃ©ation des fichiers rÃ©ussi")
    finally:
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)

def test_empty_data():
    """Teste la gestion des donnÃ©es vides"""
    print("\nğŸ§ª Test avec donnÃ©es vides...")

    empty_df = pd.DataFrame()
    output_dir = "test_artifacts"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # ExÃ©cution
        try:
            error_analysis(LinearRegression(), empty_df, empty_df, empty_df, empty_df, output_dir)
            print("âŒ Le test aurait dÃ» lever une exception")
            assert False
        except ValueError:
            print("âœ… Test avec donnÃ©es vides rÃ©ussi (exception levÃ©e comme prÃ©vu)")
    finally:
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)

def test_metrics_values():
    """Teste les valeurs des mÃ©triques"""
    print("\nğŸ§ª Test des valeurs des mÃ©triques...")

    # PrÃ©paration
    X_train, X_test, y_train, y_test, model = create_test_data()
    output_dir = "test_artifacts"
    os.makedirs(output_dir, exist_ok=True)

    try:
        # ExÃ©cution
        metrics = error_analysis(model, X_train, y_train, X_test, y_test, output_dir)

        # VÃ©rification
        assert 0 <= metrics['Train R2'] <= 1, "âŒ Train R2 hors limites"
        assert 0 <= metrics['Test R2'] <= 1, "âŒ Test R2 hors limites"
        assert metrics['Train RMSE'] > 0, "âŒ Train RMSE nÃ©gatif"
        assert metrics['Test RMSE'] > 0, "âŒ Test RMSE nÃ©gatif"
        assert metrics['Train MAE'] > 0, "âŒ Train MAE nÃ©gatif"
        assert metrics['Test MAE'] > 0, "âŒ Test MAE nÃ©gatif"

        print("âœ… Test des valeurs des mÃ©triques rÃ©ussi")
    finally:
        if os.path.exists(output_dir):
            shutil.rmtree(output_dir)

if __name__ == '__main__':
    print("ğŸš€ DÃ©marrage des tests...")

    try:
        test_error_analysis_output_structure()
        test_file_creation()
        test_empty_data()
        test_metrics_values()
        print("\nâœ¨ Tous les tests ont rÃ©ussi !")
    except AssertionError as e:
        print(f"\nâŒ Ã‰chec des tests : {str(e)}")
    except Exception as e:
        print(f"\nâŒ Erreur inattendue : {str(e)}")