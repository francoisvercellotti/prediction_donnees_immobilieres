{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import cross_validate, train_test_split, TimeSeriesSplit\n",
    "import numpy as np\n",
    "from settings import PROJECT_PATH, CLASSIFICATION_TARGET, REGRESSION_TARGET\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_parquet(os.path.join(PROJECT_PATH, 'data/interim/transactions_post_feature_engineering.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['prix', 'vefa', 'surface_habitable', 'latitude', 'longitude', 'mois_transaction', 'annee_transaction', 'en_dessous_du_marche', 'prix_m2_moyen_mois_precedent', 'nb_transactions_mois_precedent', 'ville_demandee', 'type_batiment_Maison', 'nom_region_Nouvelle-Aquitaine', 'nom_region_Occitanie', \"nom_region_Provence-Alpes-Côte d'Azur\", 'nom_region_Île-de-France']\n",
      "['surface_habitable', 'latitude', 'longitude', 'mois_transaction', 'annee_transaction', 'prix_m2_moyen_mois_precedent', 'nb_transactions_mois_precedent']\n"
     ]
    }
   ],
   "source": [
    "with open (\"../preprocessing/features_used/features_name.json\", \"r\") as f :\n",
    "    features_name = json.load (f)\n",
    "\n",
    "with open(\"../preprocessing/features_used/numerical_features.json\", \"r\") as f :\n",
    "    numerical_features= json.load(f)\n",
    "print(features_name)\n",
    "print(numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vefa', 'ville_demandee', 'type_batiment_Maison', 'nom_region_Nouvelle-Aquitaine', 'nom_region_Occitanie', \"nom_region_Provence-Alpes-Côte d'Azur\", 'nom_region_Île-de-France']\n"
     ]
    }
   ],
   "source": [
    "categorial_features = [col for col in features_name if col not in numerical_features and col not in [REGRESSION_TARGET,CLASSIFICATION_TARGET]]\n",
    "print(categorial_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'échantillons du trainset: 374492\n",
      "proportion de biens en dessous du marché dans le trainset:0.37\n",
      "Nombre d'échantillons du testset: 93623\n",
      "proportion de biens en dessous du marché dans le testset:0.37\n"
     ]
    }
   ],
   "source": [
    "# Copie du Df d'origine\n",
    "df = data.copy()\n",
    "# Séparation du Df en trainset et testset\n",
    "trainset,testset= train_test_split(df, test_size=0.2, random_state=0)\n",
    "\n",
    "print(f'Nombre d\\'échantillons du trainset: {trainset.shape[0]}')\n",
    "print(f\"proportion de biens en dessous du marché dans le trainset:{round(trainset['en_dessous_du_marche'].value_counts(normalize=True)[1],2)}\")\n",
    "print(f'Nombre d\\'échantillons du testset: {testset.shape[0]}')\n",
    "print(f\"proportion de biens en dessous du marché dans le testset:{round(testset['en_dessous_du_marche'].value_counts(normalize=True)[1],2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= trainset.drop([REGRESSION_TARGET,CLASSIFICATION_TARGET],axis=1)\n",
    "y_train_regression = trainset[REGRESSION_TARGET]\n",
    "y_train_classification = trainset[CLASSIFICATION_TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test= testset.drop([REGRESSION_TARGET,CLASSIFICATION_TARGET],axis=1)\n",
    "y_test_regression = testset[REGRESSION_TARGET]\n",
    "y_test_classification = testset[CLASSIFICATION_TARGET]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_cross_validation(\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    model,\n",
    "    cross_val_type,\n",
    "    scoring_metrics: tuple,\n",
    "    groupes=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Effectue une validation croisée pour un modèle donné et retourne les scores\n",
    "    d'entraînement et de test, ainsi que le modèle ajusté.\n",
    "\n",
    "    Paramètres :\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Les données d'entrée (features) au format pandas DataFrame.\n",
    "    y : pd.Series\n",
    "        Les étiquettes ou cibles au format pandas Series.\n",
    "    modele : object\n",
    "        Modèle d'apprentissage supervisé (ex : un modèle sklearn) à valider.\n",
    "    cross_val_type : int, générateur ou objet de type cross-validation\n",
    "        La stratégie de validation croisée à utiliser (ex : KFold, StratifiedKFold).\n",
    "    scoring_metrics : tuple\n",
    "        Les métriques d'évaluation pour la validation croisée\n",
    "        (ex : ('accuracy', 'f1')).\n",
    "    groupes : array-like, optionnel\n",
    "        Groupes utilisés pour certaines stratégies de validation croisée\n",
    "        comme GroupKFold.\n",
    "\n",
    "    Retourne :\n",
    "    ---------\n",
    "    scores : dict\n",
    "        Les scores obtenus pour chaque métrique durant la validation croisée.\n",
    "    scores_dict : dict\n",
    "        Moyennes et écarts-types des scores pour chaque métrique.\n",
    "    modele : object\n",
    "        Le modèle ajusté sur l'ensemble complet des données (X, y).\n",
    "\n",
    "    Exemple d'appel :\n",
    "    -----------------\n",
    "    >>> from sklearn.ensemble import RandomForestClassifier\n",
    "    >>> from sklearn.model_selection import KFold\n",
    "    >>> X = pd.DataFrame({'feature1': [1, 2, 3], 'feature2': [4, 5, 6]})\n",
    "    >>> y = pd.Series([0, 1, 0])\n",
    "    >>> modele = RandomForestClassifier()\n",
    "    >>> cross_val_type = KFold(n_splits=3)\n",
    "    >>> scoring_metrics = ('accuracy', 'f1')\n",
    "    >>> scores, scores_dict, model = effectuer_validation_croisée(X, y, model, cross_val_type, scoring_metrics)\n",
    "    \"\"\"\n",
    "    # Effectuer la validation croisée\n",
    "    scores = cross_validate(\n",
    "        model,\n",
    "        X.values,  # Convertir DataFrame en tableau NumPy\n",
    "        y.values,  # Convertir Series en tableau NumPy\n",
    "        cv=cross_val_type,\n",
    "        return_train_score=True,\n",
    "        return_estimator=True,\n",
    "        scoring=scoring_metrics,\n",
    "        groups=groupes,\n",
    "    )\n",
    "\n",
    "    # Calculer les moyennes et écarts-types pour chaque métrique\n",
    "    scores_dict = {}\n",
    "    for metrique in scoring_metrics:\n",
    "        scores_dict[\"moyenne_train_\" + metrique] = np.mean(scores[\"train_\" + metrique])\n",
    "        scores_dict[\"ecart_type_train_\" + metrique] = np.std(scores[\"train_\" + metrique])\n",
    "        scores_dict[\"moyenne_test_\" + metrique] = np.mean(scores[\"test_\" + metrique])\n",
    "        scores_dict[\"ecart_type_test_\" + metrique] = np.std(scores[\"test_\" + metrique])\n",
    "\n",
    "    # Ajuster le modèle sur l'ensemble des données\n",
    "    model.fit(X.values, y.values)\n",
    "\n",
    "    return scores, scores_dict, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(os.path.join(PROJECT_PATH,\"mlruns\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"regression_models\"\n",
    "experiment_tags = {\n",
    "        \"phase\": \"Model_Comparison\",\n",
    "        \"revision_de_donnees\": \"v1\",\n",
    "        \"date_de_construction\": \"Janvier 2025\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient, set_experiment\n",
    "\n",
    "def configure_mlflow_experiment(experiment_name: str, experiment_tags: dict = None) -> str:\n",
    "    \"\"\"\n",
    "    Configure une expérience MLflow. Crée l'expérience si elle n'existe pas déjà et la définit comme active.\n",
    "\n",
    "    Args:\n",
    "        experiment_name (str): Le nom de l'expérience MLflow.\n",
    "        experiment_tags (dict, optional): Un dictionnaire de tags pour l'expérience. Par défaut, aucun tag.\n",
    "\n",
    "    Returns:\n",
    "        str: L'ID de l'expérience configurée.\n",
    "    \"\"\"\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        # Tente de créer une nouvelle expérience\n",
    "        experiment_id = client.create_experiment(name=experiment_name, tags=experiment_tags)\n",
    "        print(f\"Nouvelle expérience créée avec l'ID : {experiment_id}\")\n",
    "    except Exception as e:\n",
    "        # Si l'expérience existe déjà, récupère son ID\n",
    "        print(f\"L'expérience existe déjà : {str(e)}\")\n",
    "        experiment = client.get_experiment_by_name(experiment_name)\n",
    "        if experiment is None:\n",
    "            raise ValueError(f\"L'expérience {experiment_name} n'a pas pu être récupérée.\")\n",
    "        experiment_id = experiment.experiment_id\n",
    "\n",
    "    # Définir l'expérience active\n",
    "    set_experiment(experiment_name)\n",
    "    print(f\"Expérience définie : {experiment_name} (ID : {experiment_id})\")\n",
    "\n",
    "    return experiment_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_log_model_with_mlflow(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    experiment_name,\n",
    "    run_name,\n",
    "    model_params,\n",
    "    tags,\n",
    "    cross_val_type,\n",
    "    scoring_metrics,\n",
    "    groups=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Entraîne un modèle, effectue une validation croisée avec `perform_cross_validation`,\n",
    "    enregistre les résultats et le modèle avec MLflow.\n",
    "\n",
    "    Paramètres :\n",
    "    ----------\n",
    "    model : object\n",
    "        Modèle d'apprentissage supervisé (ex : un modèle sklearn).\n",
    "    X_train : pd.DataFrame\n",
    "        Les données d'entrée pour l'entraînement.\n",
    "    y_train : pd.Series\n",
    "        Les étiquettes ou cibles.\n",
    "    experiment_name : str\n",
    "        Nom de l'expérience MLflow.\n",
    "    run_name : str\n",
    "        Nom du run dans MLflow.\n",
    "    model_params : dict\n",
    "        Paramètres du modèle.\n",
    "    tags : dict\n",
    "        Tags à ajouter au run MLflow.\n",
    "    cross_val_type : int, générateur ou objet de type cross-validation\n",
    "        La stratégie de validation croisée à utiliser (ex : KFold, StratifiedKFold).\n",
    "    scoring_metrics : tuple\n",
    "        Les métriques d'évaluation pour la validation croisée\n",
    "        (ex : ('accuracy', 'f1')).\n",
    "    groups : array-like, optionnel\n",
    "        Groupes utilisés pour certaines stratégies de validation croisée\n",
    "        comme GroupKFold.\n",
    "\n",
    "    Retourne :\n",
    "    ---------\n",
    "    scores_dict : dict\n",
    "        Moyennes et écarts-types des scores pour chaque métrique.\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    import mlflow.sklearn\n",
    "\n",
    "    # Définir ou créer l'expérience MLflow\n",
    "    from mlflow.tracking import MlflowClient\n",
    "\n",
    "    client = MlflowClient()\n",
    "    try:\n",
    "        experiment_id = client.create_experiment(name=experiment_name)\n",
    "        print(f\"Nouvelle expérience créée avec l'ID : {experiment_id}\")\n",
    "    except Exception:\n",
    "        experiment = client.get_experiment_by_name(experiment_name)\n",
    "        experiment_id = experiment.experiment_id\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    # Démarrer un nouveau run dans MLflow\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        # Ajouter les tags au run\n",
    "        mlflow.set_tags(tags)\n",
    "\n",
    "        # Effectuer la validation croisée\n",
    "        scores, scores_dict, trained_model = perform_cross_validation(\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            model=model,\n",
    "            cross_val_type=cross_val_type,\n",
    "            scoring_metrics=scoring_metrics,\n",
    "            groupes=groups,\n",
    "        )\n",
    "\n",
    "        # Enregistrer les paramètres du modèle\n",
    "        for param_name, param_value in model_params.items():\n",
    "            mlflow.log_param(param_name, param_value)\n",
    "\n",
    "        # Enregistrer les métriques\n",
    "        mlflow.log_metrics(scores_dict)\n",
    "\n",
    "\n",
    "\n",
    "        # Créer la signature du modèle à partir des données\n",
    "        signature = infer_signature(X_train, y_train)\n",
    "\n",
    "        # Exemple d'entrée pour le modèle\n",
    "        input_example = X_train.head(1)\n",
    "        mlflow.sklearn.log_model(trained_model, \"model\", input_example=input_example, signature=signature)\n",
    "\n",
    "        # Enregistrement du DataFrame d'entraînement en tant qu'artefact\n",
    "        trainset_file = \"trainset.csv\"\n",
    "        X_train.to_csv(trainset_file, index=False)\n",
    "        mlflow.log_artifact(trainset_file)\n",
    "\n",
    "        # Retourner les scores\n",
    "        return scores_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L'expérience existe déjà : Experiment 'regression_models' already exists.\n",
      "Expérience définie : regression_models (ID : 203965422485053459)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'203965422485053459'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configure_mlflow_experiment(experiment_name, experiment_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/mrgxl/Documents/cours françois/exercice python/Machine Learning/prediction_donnees_immobilieres/.venv/lib/python3.11/site-packages/mlflow/types/utils.py:435: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/mrgxl/Documents/cours françois/exercice python/Machine Learning/prediction_donnees_immobilieres/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores obtenus avec GradientBoostingRegressor : {'moyenne_train_neg_root_mean_squared_error': -119221.08607932506, 'ecart_type_train_neg_root_mean_squared_error': 7001.7119029195055, 'moyenne_test_neg_root_mean_squared_error': -150040.32884426275, 'ecart_type_test_neg_root_mean_squared_error': 3880.969165439915, 'moyenne_train_neg_mean_absolute_error': -57342.240645698934, 'ecart_type_train_neg_mean_absolute_error': 1177.5078774203898, 'moyenne_test_neg_mean_absolute_error': -60712.93732574068, 'ecart_type_test_neg_mean_absolute_error': 585.8270718642066, 'moyenne_train_r2': 0.8195443484908977, 'ecart_type_train_r2': 0.01984817669330034, 'moyenne_test_r2': 0.7233680637843023, 'ecart_type_test_r2': 0.010799728863940154}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Configuration des paramètres spécifiques au GradientBoostingRegressor\n",
    "model_params = {\n",
    "    \"n_estimators\": 150,\n",
    "    \"learning_rate\": 0.07,\n",
    "    \"max_depth\": 5,\n",
    "    \"min_samples_split\": 4,\n",
    "    \"min_samples_leaf\": 2,\n",
    "    \"subsample\": 0.9,\n",
    "    \"random_state\": 42\n",
    "}\n",
    "\n",
    "# Initialisation du modèle\n",
    "gradient_boosting_model = GradientBoostingRegressor(**model_params)\n",
    "\n",
    "# Configuration des tags pour MLflow\n",
    "tags = {\n",
    "    \"Experiment_Type\": \"models_comparison\",\n",
    "    \"Phase\": \"baseline\",\n",
    "    \"Model_Type\": \"Gradient Boosting Regressor\",\n",
    "    \"Task\": \"Regression\",\n",
    "    \"Run_Type\": \"Tunned\",\n",
    "    \"Solver\": \"Gradient Boosting\",\n",
    "    \"Dataset\": \"trainset\",\n",
    "    \"CV_Method\": \"TimeSeriesSplit\",\n",
    "    \"CV_Folds\": 5,\n",
    "}\n",
    "\n",
    "# Validation croisée\n",
    "cross_val_type = TimeSeriesSplit(5)\n",
    "scoring_metrics = [\"neg_root_mean_squared_error\", \"neg_mean_absolute_error\", \"r2\"]\n",
    "\n",
    "# Appel de la fonction\n",
    "run_name = \"gradient_boosting_regressor_run_tunned1\"\n",
    "\n",
    "scores_dict = train_and_log_model_with_mlflow(\n",
    "    model=gradient_boosting_model,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train_regression,  # Variable cible pour la régression\n",
    "    experiment_name=\"regression_models\",\n",
    "    run_name=run_name,\n",
    "    model_params=model_params,\n",
    "    tags=tags,\n",
    "    cross_val_type=cross_val_type,\n",
    "    scoring_metrics=scoring_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Scores obtenus avec GradientBoostingRegressor : {scores_dict}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
